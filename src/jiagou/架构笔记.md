---
title: 架构笔记
date: 2026-01-27
icon: stash:cloud
---

## 架构笔记

我们的一个系统肯定是很复杂的，要想将系统的复杂度降低一个很好的方案就是进行分层。一个系统的复杂度其实也是存在两个方面，第一个方面是：业务复杂度，第二个方面是：技术复杂度。所以这就对应出现两种架构图：**业务架构图、技术架构图**。

业务架构图就需要你真正的根据你的经验+你对业务的理解来进行业务的划分和架构了。这里就着重说一下技术架构。

其实对于技术架构来说大家不要想的太复杂，我个人认为只要你有足够的**技术广度**，你就可以设计一个比较好的架构。首先你要知道一个系统一般分为几层、每一层比较成熟和常用的组件有什么，那么你就可以根据你公司业务来进行选择，设计出一个符合你们公司业务的架构图。

下面是我画的一个电商系统的基本技术架构图，用它来说明一个系统一般分为几层，然后我后面会逐层说一下一些常见的组件和解决方案如何选择。

![电商系统架构图.png](https://s3.bmp.ovh/2026/01/27/HvcTA0hD.png)

---

### 访问层

访问层用于**Web 接入**、**反向代理**、**负载均衡**等。目前比较常见的两种产品：LVS、Nginx

#### LVS

LVS 是作用在 OSI7 层网络模型中的网络层（IP 层）的，采用**IP 负载均衡技术**和**内容请求分发技术**来实现将客户端的请求分发到服务器集群中，并且可以检测和自动清除出现故障的服务器，会将服务器集群与客户端进行屏蔽，整个服务器集群对客户端是**透明**的，所有公网 IP 都是与 LVS 的 IP 进行交互。

**LVS 一般有三种不同模式，分别是 LVS/NAT、LVS/DR、LVS/TUN**

1. LVS/NAT
   LVS/NAT 模式的工作过程：LVS 服务器接收到客户端的请求，请求头中有客户端的 IP 地址和 LVS 服务器的 IP 地址。但是**LVS 在将请求转发到服务器集群的时候修改了请求头**，**将自己的 IP 地址作为源 IP 地址**，**将服务器的 IP 地址作为目标 IP 地址**来进行发送。发送数据之后 LVS 服务器**不会与服务器断开 Socket 连接**而是需要**等待服务器返回响应**。服务器处理完毕之后会**将 LVS 的 IP 地址作为目标 IP 地址来进行与 LVS 服务器的通信**，LVS 服务器接收到响应之后将请求再发给客户端。

   优点：LVS 服务器部署简单

   缺点：**LVS 服务器成为系统的瓶颈**，**不适合高并发场景**，LVS 需要等待接收服务器的响应，LVS 即接收客户端的请求也接收服务器的响应。

2. LVS/DR

   LVS/DR 模式的工作过程：LVS 服务器接收到客户端的请求之后，**不会修改请求头中的 IP 信息**，但是**会修改请求头中的 MAC 信息**，将源 MAC 地址由客户端的 MAC 地址**改成 LVS 服务器的 MAC 地址**，当 LVS 服务器将请求发给服务器的时候**与 LVS/NAT 模式相比就不需要保持 socket 连接等待响应返回了**，此时就可以断开 Socket 连接，因为**服务器可以根据请求头中的 IP 信息直接将请求转发给客户端**了。

   优点：LVS 服务器不再是性能瓶颈，LVS 只负责发送请求，不需要接收服务器的响应，性能提升

   缺点：因为是修改了请求头中的 MAC 地址，所以**必须保证 LVS 服务器与服务器集群在同一个内网中**，也就是在同一个机房中。

3. LVS/TUN

​ LVS/TUN 模式的工作流程：客户端发送请求给 LVS，LVS 在将请求发给服务器的时候会进行一次处理：**将原始的 IP 包作为 payload 进行封装**，**封装到新的 IP 包中**，这个**新的 IP 包的源 IP 地址是 LVS 的 IP，目标 IP 地址是服务器的 IP 地址**。服务器在接收到 LVS 的请求之后，**通过隧道协议进行解包**，**得到客户端的 IP 地址**。因为 payload 中这个原始的 IP 包的源 IP 地址是客户端的 IP 地址，这样服务器可以直接将请求发给客户端。

​ 优点： LVS 不需要等待服务器返回响应，性能高；不借助 MAC 地址，所以 LVS 和服务器集群不需要在同一个内网中

​ 缺点：服务器必须支持 IPIP（隧道协议）

![LVS三种模式.png](https://s3.bmp.ovh/2026/01/28/uTzen0gh.png)

::: tip

在公司中一般只有访问量很大的公司一些大厂会上 LVS，因为 LVS 是比较贵的

:::

**LVS 负载到服务器的负载算法**

1. 无脑轮询
2. 加权轮询
3. 最少连接数优先
   LVS 会记录每个服务器和它的连接次数，每次会优先选择连接次数最少的服务器，也就是此时最不繁忙的服务器
4. 加权最少连接数
   在最少连接数优先的基础上，给每个服务器一个权重
5. IP_HASH
   根据客户端的 IP 执行一个 Hash 算法，然后利用得到的哈希值对服务器的数量取余去决定将请求转发到哪个服务器。可以实现固定 IP 转发到固定服务器。
6. IP_HASH_GROUP
   是对 IP_HASH 算法的改良，对客户端的 IP 进行 HASH 算法得到一个 Group 的映射，这个 Group 中对应一组服务器，对这组服务器进行轮询

::: tip info

扩展：有 IP_HASH,那其实也可以有 URL_HASH,根据客户端请求的 URL 来进行 HASH，可以用在**将登陆请求固定发到同一台服务器的场景**。

:::

**LVS 高可用**

我们在部署 LVS 服务器的时候应该也要避免单机部署，因为单机部署很容易出现单点故障，我们一般也是需要**集群部署**的。比如使用<font color=red>**Keepalived+VIP**</font>方案。

按照主备来进行部署，备机 LVS 对主机进行健康检测，当备机发现主机故障的时候能够将 VIP 对应的真实 IP 由 LVS 主机 IP 切换到 LVS 备机 IP

::: tip

**补充： VIP（虚拟 IP）: 客户端与 LVS 进行通信的时候是和这个暴露在公网的虚拟 VIP 进行通信的，虚拟 VIP 又和真实 LVS 服务器的 IP 进行映射。**

:::

---

#### Nginx

Nginx 可以分为两类，**接入层 Nginx**、**应用层 Nginx**。为什么 Nginx 可以分为两类呢，因为 Nginx 属于 OSI 七层模型中的**应用层**，所以它比 LVS 具有更多的功能。但是**Nginx 的性能是比 LVS 差**的，因为 Nginx 作用于应用层而 LVS 作用于网络层，所以 Nginx 的花销是比 LVS 大的，这就导致了 Nginx 的性能比 LVS 差，但是 Nginx 的整体性能依然是很强的。

Nginx 的功能主要有下面一些：

1. **请求解析**
2. **负载均衡**
3. **缓存调度**
4. **动静分离**
5. **授权认证**
6. **接入处理**
7. **业务逻辑**
8. **响应处理**
9. **压缩技术**

甚至 Nginx 中还可以进行限流、降级....

正是因为有这么多的功能，所以 Nginx 的应用才会分为两类。对于大部分的中小企业来说，他们系统的并发量不是特别大，基本上使用一个 Nginx 集群来作为**访问层**就可以满足整个系统的需求，再加上 Nginx 的成本比 LVS 少的多，所以也导致 Nginx 现在是访问层架构设计中使用最多的一种方案。

**Nginx 中的负载均衡算法**

1. 无脑轮询
2. 加权轮询
3. 最小连接优先
4. 加权最小连接优先
5. IP_HASH

你看到这里是不是发现了，LVS 的负载均衡算法在 Nginx 中也是都支持的。

**接入层 Nginx 的主要职责**

- 请求解析
- 请求业务路由
- **业务**负载均衡
- 响应压缩

**应用层 Nginx 的主要职责**

- **应用**负载均衡

- 缓存调度
- 授权认证
- 业务逻辑
- 业务限流
- 业务降级

**Nginx 高性能的原理**

1. **Nginx 使用 Master-Worker 多进程模式**
2. **基于 epoll 的事件驱动**
3. 使用了协程

4. **可以使用零拷贝来优化磁盘 I/O**

下面我画了一个 Nginx 的简单的原理图

![Nginx高性能原理.png](https://s3.bmp.ovh/2026/01/29/Xs4xlQjV.png)

下面我会基于这个图来说一下高性能的原理：

首先 Nginx 采用了**Master-Worker 多进程的进程模式**，有一个 Master，他可以管理多个 Worker,这个 Worker 的数量是你自己可以设置的，**一般不要超过 CPU 的核数**。**Master 进程不处理请求，真正的工作进程是 Worker 进程**。

**每一个 Worker 进程都是单线程的**，为什么设计单线程呢？因为**可以避免锁竞争+上下文切换**。虽然 Worker 进程是单线程的但是**一个 Worker 可以连接多个 Scoket**，这个 Socket 的数量也是你自己可以设置的。在 Worker 和 Socket 的连接这里就使用了**多路复用模型**。

**Worker 进程是运行在用户态的**。**Socket 是运行在内核态的**。

在 Linux 系统中，Nginx 的 Worker 进程会使用**epoll 给内核态发送一个请求询问内核态是否有 Socket 就绪**，但是发送完成之后**Worker 进程并不会阻塞**在这里等待内核态的返回而是可以继续执行其他任务。当内核态的**任一 Socket 就绪之后**会使用**epoll 的通知机制**来通知用户态的 Worker 进程数据就绪，用户态的 Worker 接收到 epoll 通知后就会处理 Socket 的请求。

虽然每个 Worker 是单线程运行的，但是**线程内部使用了协程来提高效率**。

::: tip info

补充：

协程比线程高性能的原因是什么：当协程遇到阻塞之后会自己**主动释放**，然后其他的协程接着跑，而线程遇到阻塞之后不会自己主动释放需要**等待 CPU 调度**。所以协程阻塞后会迅速切换另一个协程去执行任务，而线程咋会卡在这里浪费时间。

**epoll: Linux 提供的一种高效 I/O 多路复用机制，用来在单线程中同时监听大量文件描述符的就绪事件。**

:::

---

### 网关层
