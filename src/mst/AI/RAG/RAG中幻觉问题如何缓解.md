---
title: RAG中幻觉问题如何缓解？
icon: bxs:coffee-alt
author: Carl
date: 2026-01-24
order: 2



---

## RAG中幻觉问题如何缓解？



### 幻觉问题出现的原因

在回答幻觉问题如何缓解之前，要先明确RAG中幻觉出现的原因，幻觉出现一般是因为下面的三个原因：

1. 检索质量低，导致喂给大模型的上下文本身就是错的
2. 上下文的匹配度低，导致大模型的理解出现了偏差
3. 当大模型根据上下文无法很好的回答问题时，大模型偏向使用自己预训练的数据来回答问题



### 优化方案

知道了出现幻觉的原因，那么我们就知道该如何进行优化了，RAG中幻觉问题的缓解要**从多个层面来优化**。

1. **检索层优化**

   使用混合检索来进行优化，**稠密向量擅长进行语义理解但是对专有名词、日期、数字这类精准匹配的内容不敏感，所以就可以使用稀疏检索和关键词匹配来对关键词进行匹配**。另外，我们在工程中可能有这种操作：我们可能会**召回更多的topK**然后对to**pK进行关键词匹配**，最后再**借助于重排序LLM进行重排序**。

   在分词阶段，我们要适当的调整chunk size的大小和overlap的大小来防止一个完整的语义被切到多个chunk中导致大模型不能理解。

2. **生成层优化**

   **Prompt优化**

   针对大模型可能会使用自己预训练的数据来回答的这个问题，我们就需要使用提示词工程来对Prompt进行优化了。比如我们可以**给大模型设定一个角色，然后对大模型的输出格式也进行限制**。

   下面我给一个可以借鉴的提示词模板

   ```java
   String promptTemplate="""
       你是一个严谨的知识助手，负责根据提供的文档回答用户问题。
       【重要规则】
       仅使用下方参考文档中的信息进行回答，不得使用你的预训练知识。如果文档中没有相关的知识，必须明确回复
       “根据现有文档无法回答该问题。”。回答时必须在每个关键信息后面标注信息来源，格式为[doc_X]。保持回答简洁准确，不要过度推理或猜测。
       【参考文档】
       %s
       
       【用户问题】
       %s
       
       【回答要求】
       请按以下JSON格式输出:
       {
   		"answer": "你的回答内容",
           "confidence": 0.0~1.0的置信度分数,
           "citations": ["doc_1","doc_2"]
       }
       """
   ```

   **温度调整**

   我们可以将温度调低，比如0.1~0.2,较低的温度可以让大模型没有那么高的创造性，所以生成的答案会更加偏向于使用上下文中的内容来进行回答，会减少幻觉问题的出现。

   
   
   **引用溯源**
   
   什么是引用溯源呢，其实很简单，就是：**大模型通过上下文中的chunk生成答案之后，要告诉用户这个回答是基于哪个文档中的哪个内容来得出的**。
   
   在很多场景中我们是需要引用溯源的，比如**金融、医疗**等场景中，用户进行提问的时候我们如果把原始文档中的内容一块展示给用户，可能会让用户更好的相信我们的输出内容。
   
   **引用溯源不是模型能力而是一个工程能力**。
   
   其实RAG中引用溯源的实现核心是：“**在 RAG 全流程中保留并传递源文档的元数据**”。这个具体到实现上就是在进行文本切分的时候，针对每一个chunk我们都要设置元数据，元数据里面要进行保存比较具体的信息，比如：**文档片段的id、文档名称、title、页码**等。
   
   这样LLM在生成内容的时候，上下文的信息中就保留了元数据，就可以通过元数据得到这个文档片段来自哪个文档中的哪个位置。
   
   然后我们可以**限制大模型的输出格式**。让**大模型在输出答案的时候将“答案”与“来源”绑定**。
   
   比如我们可以指定大模型的输出格式：

```text
请按如下 JSON 格式回答：
{
  "answers": [
    {
      "text": "你的回答",
      "citations": ["doc_12_chunk_3"]
    }
  ]
}

```

​	因为引用溯源是将生成的答案和原文进行绑定，所以也可以缓解幻觉的产生。